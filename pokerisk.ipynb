{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRAT_SIZE = 10\n",
    "basic_sim_strat = tf.Variable(tf.random.uniform((STRAT_SIZE, STRAT_SIZE, STRAT_SIZE)), trainable=True)\n",
    "table_round_strat = tf.Variable(tf.random.uniform((STRAT_SIZE, STRAT_SIZE, STRAT_SIZE)), trainable=True)\n",
    "#(my_prob, largest_bet, total_balance_bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32768\n",
    "TABLE_SIZE = 6\n",
    "\n",
    "@tf.function\n",
    "def discret(x):\n",
    "    return tf.cast(x * STRAT_SIZE, tf.int32)\n",
    "\n",
    "@tf.function\n",
    "def run_basic_sim_batch(strat):\n",
    "    samp_idx = tf.concat([tf.expand_dims(tf.range(BATCH_SIZE), -1), tf.expand_dims(tf.cast(tf.random.uniform((BATCH_SIZE,)) * TABLE_SIZE, tf.int32), -1)], axis=-1)\n",
    "    probs = tf.random.uniform((BATCH_SIZE, TABLE_SIZE))\n",
    "    strats = tf.minimum(tf.maximum(tf.gather(strat, discret(probs), axis=0), 0.0), 1.0)\n",
    "    strats = tf.where((samp_idx[:, 1, tf.newaxis] == tf.repeat(tf.expand_dims(tf.range(TABLE_SIZE), 0), BATCH_SIZE, axis=0))[:, :, tf.newaxis, tf.newaxis], strats, tf.stop_gradient(strats))\n",
    "    pots = tf.zeros((BATCH_SIZE,))\n",
    "    larg = tf.zeros((BATCH_SIZE,))\n",
    "    stacks = tf.ones((BATCH_SIZE, TABLE_SIZE))\n",
    "    playing = tf.zeros((BATCH_SIZE,))\n",
    "    risks = tf.zeros((BATCH_SIZE, 0))\n",
    "    for i in range(TABLE_SIZE):\n",
    "        act = tf.gather_nd(strats, tf.concat([\n",
    "            tf.expand_dims(tf.range(BATCH_SIZE), -1),\n",
    "            tf.ones((BATCH_SIZE, 1), dtype=tf.int32) * i,\n",
    "            tf.expand_dims(discret(pots/TABLE_SIZE), -1),\n",
    "            tf.expand_dims(discret(larg), -1)\n",
    "        ], axis=-1))\n",
    "        plays = act >= larg\n",
    "        playing += tf.cast(plays, tf.float32)\n",
    "        larg = tf.where(plays, act, larg)\n",
    "        pots = tf.where(plays, pots + stacks[:, i] * act, pots)\n",
    "        risks = tf.concat([risks, tf.expand_dims(tf.where(plays, act, 0.0), 1)], axis=1)\n",
    "    reward = stacks + ((probs/tf.reduce_sum(probs * tf.cast(probs != 0, tf.float32), axis=-1, keepdims=True)) * tf.expand_dims(playing, 1) - 1) * risks\n",
    "    samp = tf.gather_nd(reward, samp_idx)\n",
    "    return -tf.reduce_mean(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 100\n",
    "optimizer = tf.keras.optimizers.Adam(.1)\n",
    "for i in (range(ITERS)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        neg_risks = run_basic_sim_batch(basic_sim_strat)\n",
    "        print(str(i) + \"/\" + str(ITERS), -neg_risks.numpy(), end=\"\\r\")\n",
    "    grads = tf.convert_to_tensor(tape.gradient(neg_risks, basic_sim_strat))\n",
    "    optimizer.apply_gradients(zip(grads, [basic_sim_strat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_diverse_sim_batch(strat):\n",
    "    samp_idx = tf.concat([tf.expand_dims(tf.range(BATCH_SIZE), -1), tf.expand_dims(tf.cast(tf.random.uniform((BATCH_SIZE,)) * TABLE_SIZE, tf.int32), -1)], axis=-1)\n",
    "    probs = tf.random.uniform((BATCH_SIZE, TABLE_SIZE))\n",
    "    strats = tf.minimum(tf.maximum(tf.gather(tf.where(tf.random.uniform(tf.shape(strat)) > .1, strat, tf.random.uniform(tf.shape(strat))), discret(probs), axis=0), 0.0), 1.0)\n",
    "    strats = tf.where((samp_idx[:, 1, tf.newaxis] == tf.repeat(tf.expand_dims(tf.range(TABLE_SIZE), 0), BATCH_SIZE, axis=0))[:, :, tf.newaxis, tf.newaxis], strats, tf.stop_gradient(strats))\n",
    "    pots = tf.zeros((BATCH_SIZE,))\n",
    "    larg = tf.zeros((BATCH_SIZE,))\n",
    "    stacks = tf.ones((BATCH_SIZE, TABLE_SIZE))\n",
    "    playing = tf.zeros((BATCH_SIZE,))\n",
    "    risks = tf.zeros((BATCH_SIZE, 0))\n",
    "    for i in range(TABLE_SIZE):\n",
    "        act = tf.gather_nd(strats, tf.concat([\n",
    "            tf.expand_dims(tf.range(BATCH_SIZE), -1),\n",
    "            tf.ones((BATCH_SIZE, 1), dtype=tf.int32) * i,\n",
    "            tf.expand_dims(discret(pots/TABLE_SIZE), -1),\n",
    "            tf.expand_dims(discret(larg), -1)\n",
    "        ], axis=-1))\n",
    "        plays = act >= larg\n",
    "        playing += tf.cast(plays, tf.float32)\n",
    "        larg = tf.where(plays, act, larg)\n",
    "        pots = tf.where(plays, pots + stacks[:, i] * act, pots)\n",
    "        risks = tf.concat([risks, tf.expand_dims(tf.where(plays, act, 0.0), 1)], axis=1)\n",
    "    reward = stacks + ((probs/tf.reduce_sum(probs * tf.cast(probs != 0, tf.float32), axis=-1, keepdims=True)) * tf.expand_dims(playing, 1) - 1) * risks\n",
    "    samp = tf.gather_nd(reward, samp_idx)\n",
    "    return -tf.reduce_mean(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 100000\n",
    "optimizer = tf.keras.optimizers.Adam(.05)\n",
    "for i in (range(ITERS)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        neg_risks = run_diverse_sim_batch(basic_sim_strat)\n",
    "        print(str(i) + \"/\" + str(ITERS), -neg_risks.numpy(), end=\"\\r\")\n",
    "    grads = tf.convert_to_tensor(tape.gradient(neg_risks, basic_sim_strat))\n",
    "    optimizer.apply_gradients(zip(grads, [basic_sim_strat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_single_table_round_batch(strat):\n",
    "    samp_idx = tf.concat([tf.expand_dims(tf.range(BATCH_SIZE), -1), tf.expand_dims(tf.cast(tf.random.uniform((BATCH_SIZE,)) * TABLE_SIZE, tf.int32), -1)], axis=-1)\n",
    "    probs = tf.random.uniform((BATCH_SIZE, TABLE_SIZE))\n",
    "    strats = tf.minimum(tf.maximum(tf.gather(strat, discret(probs), axis=0), 0.0), 1.0)\n",
    "    strats = tf.where((samp_idx[:, 1, tf.newaxis] == tf.repeat(tf.expand_dims(tf.range(TABLE_SIZE), 0), BATCH_SIZE, axis=0))[:, :, tf.newaxis, tf.newaxis], strats, tf.stop_gradient(strats))\n",
    "    pots = tf.zeros((BATCH_SIZE,))\n",
    "    larg = tf.zeros((BATCH_SIZE,))\n",
    "    stacks = tf.ones((BATCH_SIZE, TABLE_SIZE))\n",
    "    playing = tf.zeros((BATCH_SIZE,))\n",
    "    players = tf.cast(tf.zeros((BATCH_SIZE, 0)), tf.bool)\n",
    "    risks = tf.zeros((BATCH_SIZE, 0))\n",
    "    for i in range(TABLE_SIZE):\n",
    "        act = tf.gather_nd(strats, tf.concat([\n",
    "            tf.expand_dims(tf.range(BATCH_SIZE), -1),\n",
    "            tf.ones((BATCH_SIZE, 1), dtype=tf.int32) * i,\n",
    "            tf.expand_dims(discret(pots/TABLE_SIZE), -1),\n",
    "            tf.expand_dims(discret(larg), -1)\n",
    "        ], axis=-1))\n",
    "        act = tf.where(tf.math.logical_and(tf.random.uniform(tf.shape(act)) < .75, act >= larg), larg, act)\n",
    "        plays = act >= larg\n",
    "        playing += tf.cast(plays, tf.float32)\n",
    "        larg = tf.where(plays, act, larg)\n",
    "        pots = tf.where(plays, pots + stacks[:, i] * act, pots)\n",
    "        risks = tf.concat([risks, tf.expand_dims(tf.where(plays, act, 0.0), 1)], axis=1)\n",
    "        players = tf.concat([players, tf.expand_dims(plays, 1)], axis=1)\n",
    "    reward = stacks + ((probs/tf.reduce_sum(probs * tf.cast(probs != 0, tf.float32), axis=-1, keepdims=True)) * tf.expand_dims(playing, 1) - 1) * risks\n",
    "    samp = tf.gather_nd(reward, samp_idx)\n",
    "    return -tf.reduce_mean(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 500\n",
    "optimizer = tf.keras.optimizers.Adam(.1)\n",
    "for i in (range(ITERS)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        neg_risks = run_single_table_round_batch(table_round_strat)\n",
    "        print(str(i) + \"/\" + str(ITERS), -neg_risks.numpy(), end=\"\\r\")\n",
    "    grads = tf.convert_to_tensor(tape.gradient(neg_risks, table_round_strat))\n",
    "    optimizer.apply_gradients(zip(grads, [table_round_strat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"./npy\", basic_sim_strat.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
